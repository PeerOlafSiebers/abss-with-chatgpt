{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "8CmI3n549jIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: why didn't i get a response in the cell above?\n",
        "\n",
        "!pip install -U bitsandbytes\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "\n",
        "# Load Mistral 7B model and tokenizer\n",
        "model_id = \"Phanh2532/GAML-151-500\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load the tokenizer *BEFORE* using it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, add_bos_token=True, trust_remote_code=True)\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=\"\" # INCLUDE API TOKEN HERE\n",
        ")\n",
        "\n",
        "prompt = \"Write a simple GAML species \\\"Prey\\\" which represents a prey in the predator/prey cycle?\" # Replace with your desired prompt\n",
        "\n",
        "# Check for CUDA availability\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Warning: CUDA not available. Using CPU. This will be very slow.\")\n",
        "\n",
        "model.to(device) # Explicitly move the model to the chosen device\n",
        "model_input = tokenizer(prompt, return_tensors=\"pt\").to(device) # Move the input tensor as well\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=10000, repetition_penalty=1.05)[0], skip_special_tokens=True))\n",
        "    print('----------------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "Kf1fkAEUBNIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write a GAML program for the predator/prey cycle?\" # Replace with your desired prompt\n",
        "\n",
        "# Check for CUDA availability\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Warning: CUDA not available. Using CPU. This will be very slow.\")\n",
        "\n",
        "model.to(device) # Explicitly move the model to the chosen device\n",
        "model_input = tokenizer(prompt, return_tensors=\"pt\").to(device) # Move the input tensor as well\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=10000, repetition_penalty=1.05)[0], skip_special_tokens=True))\n",
        "    print('----------------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "B3PdcMR7B0vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write a GAML program to model a flu epidemic?\" # Replace with your desired prompt\n",
        "\n",
        "# Check for CUDA availability\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Warning: CUDA not available. Using CPU. This will be very slow.\")\n",
        "\n",
        "model.to(device) # Explicitly move the model to the chosen device\n",
        "model_input = tokenizer(prompt, return_tensors=\"pt\").to(device) # Move the input tensor as well\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=10000, repetition_penalty=1.05)[0], skip_special_tokens=True))\n",
        "    print('----------------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "KYPQG1gHJvtE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}